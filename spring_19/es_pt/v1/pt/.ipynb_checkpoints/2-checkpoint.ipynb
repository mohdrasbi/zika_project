{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../filtered_tweets_es.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foundPronouns(tweet):\n",
    "    pronouns = ['i', 'me', 'my', 'we', 'us', 'our', 'eu', 'mim', 'minha', 'meus', 'meu', 'minhas',\n",
    "               'n√≥s', ]\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    \n",
    "    #remove US and U.S.\n",
    "    mod_tweet = tweet.replace(\"US\", \"\")\n",
    "    mod_tweet = mod_tweet.replace(\"U.S.\", \"\")\n",
    "    \n",
    "    mod_tweet = mod_tweet.lower().replace(\"'\", \" \").split()\n",
    "    for word in mod_tweet:\n",
    "        re_word = regex.sub('', word)\n",
    "        if re_word in pronouns:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def POS(tweets, tags):    \n",
    "    tweets_tags = []\n",
    "    all_tags = set()\n",
    "    nlp = stanfordnlp.Pipeline(processors=\"tokenize,pos\", lang=\"es\")\n",
    "    for i in range(len(tweets)):\n",
    "        doc = nlp(tweets[i])\n",
    "        first_word_tag = doc.sentences[0].words[0].upos\n",
    "        all_tags.add(first_word_tag)\n",
    "        if first_word_tag in tags:\n",
    "            tweets_tags.append(tweets[i])\n",
    "    \n",
    "    print(\"all tags:\", all_tags)\n",
    "    return tweets_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(df['filtered_tweets_es'])\n",
    "tweets_pronouns = []\n",
    "for tweet in tweets:\n",
    "    if foundPronouns(tweet):\n",
    "        tweets_pronouns.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pronouns = pd.DataFrame({\"tweets_pronouns\": tweets_pronouns})\n",
    "df_pronouns.to_csv(\"stanfordnlp/tweets_pronouns.csv\")\n",
    "df_pronouns.sample(100).to_csv(\"stanfordnlp/samples/tweets_pronouns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5860\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets_pronouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets begin with a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: VERB\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/mohd/stanfordnlp_resources/es_ancora_models/es_ancora_tokenizer.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/mohd/stanfordnlp_resources/es_ancora_models/es_ancora_tagger.pt', 'pretrain_path': '/Users/mohd/stanfordnlp_resources/es_ancora_models/es_ancora.pretrain.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "all tags: {'CCONJ', 'PRON', 'ADJ', 'PROPN', 'DET', 'VERB', 'NOUN', 'ADP', 'AUX', 'PUNCT', 'SYM', 'NUM', 'INTJ', 'ADV', 'SCONJ', 'PART'}\n",
      "Length of tweets_VERB: 7198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags = ['VERB']\n",
    "tweets_tags = {}\n",
    "for tag in tags:\n",
    "    print(\"Tag: {}\".format(tag))\n",
    "    tweets_tags[\"tweets_{}\".format(tag)] = POS(tweets, tag)\n",
    "    print(\"Length of tweets_{}: {}\\n\".format(tag, len(tweets_tags[\"tweets_{}\".format(tag)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tweets_tags:\n",
    "    temp = pd.DataFrame({i: tweets_tags[i]})\n",
    "    temp.to_csv(\"stanfordnlp/{}.csv\".format(i))\n",
    "    try:\n",
    "        temp.sample(100).to_csv(\"stanfordnlp/samples/{}.csv\".format(i))\n",
    "    except:\n",
    "        temp.to_csv(\"stanfordnlp/samples/{}.csv\".format(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_VERB 7198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7198"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for i in tweets_tags:\n",
    "    print(i, len(tweets_tags[i]))\n",
    "    s += len(tweets_tags[i])\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
