{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../filtered_tweets_pt.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foundPronouns(tweet):\n",
    "    pronouns = ['i', 'me', 'my', 'we', 'us', 'our', 'eu', 'mim', 'minha', 'meus', 'meu', 'minhas',\n",
    "               'n√≥s', 'nos', 'nossa', 'nosso', 'nossos']\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    \n",
    "    #remove US and U.S.\n",
    "    mod_tweet = tweet.replace(\"US\", \"\")\n",
    "    mod_tweet = mod_tweet.replace(\"U.S.\", \"\")\n",
    "    \n",
    "    mod_tweet = mod_tweet.lower().replace(\"'\", \" \").split()\n",
    "    for word in mod_tweet:\n",
    "        re_word = regex.sub('', word)\n",
    "        if re_word in pronouns:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def POS(tweets, tags):    \n",
    "    tweets_tags = []\n",
    "    \n",
    "    for i in range(len(tweets)):\n",
    "        pos = nltk.pos_tag(word_tokenize(tweets[i]))\n",
    "        first_word_tag = pos[0][1]\n",
    "        \n",
    "        if first_word_tag in tags:\n",
    "            tweets_tags.append(tweets[i])\n",
    "        \n",
    "    return tweets_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(df['filtered_tweets_pt'])\n",
    "tweets_pronouns = []\n",
    "for tweet in tweets:\n",
    "    if foundPronouns(tweet):\n",
    "        tweets_pronouns.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pronouns = pd.DataFrame({\"tweets_pronouns\": tweets_pronouns})\n",
    "df_pronouns.to_csv(\"nltk/tweets_pronouns.csv\")\n",
    "df_pronouns.sample(100).to_csv(\"nltk/samples/tweets_pronouns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5860\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets_pronouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets begin with a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: VB\n",
      "Length of tweets_VB: 371\n",
      "\n",
      "Tag: VBD\n",
      "Length of tweets_VBD: 371\n",
      "\n",
      "Tag: VBG\n",
      "Length of tweets_VBG: 385\n",
      "\n",
      "Tag: VBN\n",
      "Length of tweets_VBN: 638\n",
      "\n",
      "Tag: VBP\n",
      "Length of tweets_VBP: 374\n",
      "\n",
      "Tag: VBZ\n",
      "Length of tweets_VBZ: 375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "tweets_tags = {}\n",
    "for tag in tags:\n",
    "    print(\"Tag: {}\".format(tag))\n",
    "    tweets_tags[\"tweets_{}\".format(tag)] = POS(tweets, tag)\n",
    "    print(\"Length of tweets_{}: {}\\n\".format(tag, len(tweets_tags[\"tweets_{}\".format(tag)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tweets_tags:\n",
    "    temp = pd.DataFrame({i: tweets_tags[i]})\n",
    "    temp.to_csv(\"nltk/{}.csv\".format(i))\n",
    "    try:\n",
    "        temp.sample(100).to_csv(\"nltk/samples/{}.csv\".format(i))\n",
    "    except:\n",
    "        temp.to_csv(\"nltk/samples/{}.csv\".format(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_VB 371\n",
      "tweets_VBD 371\n",
      "tweets_VBG 385\n",
      "tweets_VBN 638\n",
      "tweets_VBP 374\n",
      "tweets_VBZ 375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2514"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "for i in tweets_tags:\n",
    "    print(i, len(tweets_tags[i]))\n",
    "    s += len(tweets_tags[i])\n",
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
